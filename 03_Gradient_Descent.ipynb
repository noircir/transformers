{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9556da0-60d6-4e63-a5ae-1dbca8c4befa",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1432abb9-aa85-4dd7-b622-559c3765a201",
   "metadata": {},
   "source": [
    "## 1. Manual Calculations of Gradient Descent\n",
    "\n",
    "### - Prediction: Manually\n",
    "### - Gradients Computation: Manually\n",
    "### - Loss Computation: Manually\n",
    "### - Parameter updates: Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d045593-10c0-418a-97ef-c9b692afa679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60f5bbdd-d12f-4dea-820c-eb006a5dd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ce14655-5c76-4b39-a537-d859e9024de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = w*x\n",
    "# f = 2*x  # a function that multiplies any given number by 2 (we don't know the function yet)\n",
    "\n",
    "# Usually the function is not known, only the inputs (observations) and outputs. The task is to find weights to accomodate this relationship. \n",
    "# Then any other number, like 5, will also be churned through the function in the same fashion.\n",
    "\n",
    "X = np.array([1,2,3,4], dtype=np.float32) # observations\n",
    "Y = np.array([2,4,6,8], dtype=np.float32) # results for each observation\n",
    "w = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9885df-9fbc-4584-abca-33c57ffbc2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a850ac4-9efe-4539-bd50-8dc8ed3d4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = MSE\n",
    "# loss -> 0\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cf8d03f-597d-4642-b236-b36eecb74a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n"
     ]
    }
   ],
   "source": [
    "# gradient : denotes the direction of greatest change of a scalar function.\n",
    "# We can choose the error method. \n",
    "# If we choose MSE, then the derivative of MSE, with the help of the \"chain rule\" is:\n",
    "\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# chain rule (see 02_Backpropagation.ipynb) dloss/dw = 2s * x = 2x * (y_hat - y) . To find the mean, divide by N:\n",
    "# dJ/dw = 1/N * 2*(w*x - y) * x       = 1/N 2x (w*x - y)\n",
    "\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, (y_predicted - y)).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3428cdc-9949-4ea0-b0af-8056f44f5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "Prediction after training: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction: forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate derivative of the loss function\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights (negative direction wrt the gradient)\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9180554b-3e7f-4273-a24b-25e257a25056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# More iterations\n",
    "\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype=np.float32)\n",
    "w = 0.0\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction: forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    # update weights (negative direction wrt the gradient)\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f43ab-f5fd-4fd2-8de9-3fbeeac305ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e696cf7-1f6f-4169-b4d0-38659dd51931",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Manually, with Autograd for Gradient Descent\n",
    "\n",
    "### - Prediction: Manually\n",
    "### - Gradients Computation: Autograd\n",
    "### - Loss Computation: Manually\n",
    "### - Parameter updates: Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96a25a15-f886-4b30-975c-6807c060479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) # interested in derivative wrt weights w. Turning on tracking of nodes history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0c90729-a81a-432f-8b25-6b2766e17e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "Prediction after training: f(5) = 9.612\n"
     ]
    }
   ],
   "source": [
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction: forward pass\n",
    "    y_pred = forward(X)  # history: node1 (multiplied two tensors)\n",
    "    \n",
    "    # calculate loss\n",
    "    l = loss(Y, y_pred) # history: node2 (subtraction of 2 tensors) and node3 (power of 2)\n",
    "    \n",
    "    # calculate gradient of the loss function (autograd) - the node graph is by now in memory\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update weights (dl/dw = w.grad - calculates automatically since the history of nodes is tracked)\n",
    "    # we don't want to create a graph and track the nodes history on the backward pass => no_grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    # prevent gradient accumulation\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b652bf77-9c9b-4bd3-a913-ba986a62d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# not as accurate as manual -> let's increase the number of iterations\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction: forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update weights (negative direction wrt the gradient)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077dae7-3f8d-448a-8a19-21d34ba3e8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7abfc50-5452-4b34-b245-d9dc4e66f0ef",
   "metadata": {},
   "source": [
    "## 3. Gradient, Loss and Parameter updates by PyTorch\n",
    "\n",
    "### - Prediction: Manually\n",
    "### - Gradients Computation: Autograd\n",
    "### - Loss Computation: PyTorch Loss\n",
    "### - Parameter updates: PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6acaa67-8010-4103-8be3-377c12072a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# The steps of a design:\n",
    "\n",
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()  # replace manual loss function with PyTorch function\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate) # parameter are the weights (to optimize)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction: forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update weights (negative direction wrt the gradient)\n",
    "    optimizer.step()\n",
    "        \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08244a81-e942-4add-bedf-6ec07ef5ce33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e805cd0d-bd07-4091-864b-b391caa304c8",
   "metadata": {},
   "source": [
    "## 4. Gradient, Loss and Parameter updates by PyTorch\n",
    "\n",
    "### - Prediction: PyTorch Model\n",
    "### - Gradients Computation: Autograd\n",
    "### - Loss Computation: PyTorch Loss\n",
    "### - Parameter updates: PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc7acd9-4808-4749-8029-cf9ee1727572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Model input size: 1\n",
      "Model ouput size: 1\n",
      "Model weights before training: tensor([-0.2777], grad_fn=<SelectBackward0>)\n",
      "Model biases before training: -0.2588077783584595\n",
      "Prediction before training: f(5) = -1.647\n",
      "epoch 1: w = 0.077, bias: -0.140, loss = 41.92353439\n",
      "epoch 11: w = 1.564, bias: 0.351, loss = 1.11736858\n",
      "epoch 21: w = 1.807, bias: 0.419, loss = 0.05971167\n",
      "epoch 31: w = 1.849, bias: 0.419, loss = 0.03055468\n",
      "epoch 41: w = 1.860, bias: 0.409, loss = 0.02811180\n",
      "epoch 51: w = 1.865, bias: 0.397, loss = 0.02645837\n",
      "epoch 61: w = 1.869, bias: 0.385, loss = 0.02491792\n",
      "epoch 71: w = 1.873, bias: 0.374, loss = 0.02346754\n",
      "epoch 81: w = 1.877, bias: 0.363, loss = 0.02210162\n",
      "epoch 91: w = 1.880, bias: 0.352, loss = 0.02081517\n",
      "epoch 101: w = 1.884, bias: 0.342, loss = 0.01960364\n",
      "epoch 111: w = 1.887, bias: 0.331, loss = 0.01846258\n",
      "epoch 121: w = 1.891, bias: 0.322, loss = 0.01738797\n",
      "epoch 131: w = 1.894, bias: 0.312, loss = 0.01637589\n",
      "epoch 141: w = 1.897, bias: 0.303, loss = 0.01542274\n",
      "epoch 151: w = 1.900, bias: 0.294, loss = 0.01452507\n",
      "epoch 161: w = 1.903, bias: 0.285, loss = 0.01367965\n",
      "epoch 171: w = 1.906, bias: 0.277, loss = 0.01288340\n",
      "epoch 181: w = 1.909, bias: 0.269, loss = 0.01213354\n",
      "epoch 191: w = 1.911, bias: 0.261, loss = 0.01142729\n",
      "Prediction after training: f(5) = 9.822\n"
     ]
    }
   ],
   "source": [
    "# The steps of a design:\n",
    "\n",
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Training loop\n",
    "#  - forward pass: compute prediction\n",
    "#  - backward pass: gradients\n",
    "#  - update weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)  # 4 samples, 1 feature\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size) # The model now knows taht we have inputs, outputs, and want to optimize weights (model parameters)\n",
    "print(f\"Model input size: {input_size}\")\n",
    "print(f\"Model ouput size: {output_size}\")\n",
    "[w,b] = model.parameters()\n",
    "print(f\"Model weights before training: {w[0]}\") # a generator\n",
    "print(f\"Model biases before training: {b[0]}\") # a generator\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 200\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # The weights (to optimize) are the model parameters now\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction: forward pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y_pred, Y)\n",
    "    \n",
    "    # gradients\n",
    "    l.backward() # dl/dw\n",
    "    \n",
    "    # update weights (the optimizer takes care of the negative direction wrt the gradient)\n",
    "    optimizer.step()\n",
    "        \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, bias: {b[0]:.3f}, loss = {l:.8f}')\n",
    "        # print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')   \n",
    "\n",
    "# still less perfect - experiments with learning rate and n_iter\n",
    "# The overall improvement is visible, though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4049faa-8f6d-49be-9a21-c873e1f107a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b527bc-729e-4938-b3f7-e42fc3367c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can create a custom linear regression model\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f597e22-5138-4387-a6e9-cf5746df6c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1e17b-8c3a-4195-8769-15a24ad5e553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78036acf-ecce-467f-a6cb-281c84056b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296318a-4708-4d9d-b4af-0b565b33f860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977ba7a-065a-4f68-87ac-518b619a8c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f9a7f-3a9f-4a05-9bb5-d83818f82b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa449281-520f-4274-a52e-ec4728ccc020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dd644-b833-4a32-9a33-4db55a1e2cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f352c-0326-4246-b39f-57a2e13e6e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2faaec1-c3a4-4181-99c0-aed646cb4ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
